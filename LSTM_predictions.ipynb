{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporacion Favorita Grocery Sales Forecasting\n",
    "# LSTM Sequential Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "import gc \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    'train_oiled.csv', \n",
    "    dtype={'onpromotion': bool},\n",
    "    converters={'unit_sales': lambda u: np.log1p(\n",
    "        float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],\n",
    "    skiprows=range(1, 66458909)  \n",
    ")\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    \"test_oiled.csv\",\n",
    "    dtype={'onpromotion': bool},\n",
    "    parse_dates=[\"date\"]  # , date_parser=parser\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date']\n",
    ")\n",
    "df_test.columns = ['id', 'onpromotion', 'oil']\n",
    "\n",
    "items = pd.read_csv(\n",
    "    \"data/items.csv\",\n",
    ").set_index(\"item_nbr\")\n",
    "\n",
    "# LSTM will train in a recent year data\n",
    "df_2017 = df_train.loc[df_train.date>=pd.datetime(2017,1,1)]\n",
    "del df_train\n",
    "\n",
    "\"\"\"\n",
    "    Unstack time series data to represent each time series as a row\n",
    "    with store-item combination as an index;\n",
    "\"\"\"\n",
    "\n",
    "promo_2017_train = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(\n",
    "        level=-1).fillna(False)\n",
    "promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)\n",
    "promo_2017_test = df_test[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017_test.columns = promo_2017_test.columns.get_level_values(1)\n",
    "promo_2017_test = promo_2017_test.reindex(promo_2017_train.index).fillna(False)\n",
    "promo_2017 = pd.concat([promo_2017_train, promo_2017_test], axis=1)\n",
    "del promo_2017_test, promo_2017_train\n",
    "\n",
    "oil_2017_train = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"oil\"]].unstack(\n",
    "        level=-1).fillna(False)\n",
    "\n",
    "oil_2017_train.columns = oil_2017_train.columns.get_level_values(1)\n",
    "oil_2017_test = df_test[[\"oil\"]].unstack(level=-1).fillna(0)\n",
    "oil_2017_test.columns = oil_2017_test.columns.get_level_values(1)\n",
    "oil_2017_test = oil_2017_test.reindex(oil_2017_train.index).fillna(0)\n",
    "oil_2017 = pd.concat([oil_2017_train, oil_2017_test], axis=1)\n",
    "del oil_2017_test, oil_2017_train\n",
    "gc.collect()\n",
    "\n",
    "df_2017 = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n",
    "        level=-1).fillna(0)\n",
    "df_2017.columns = df_2017.columns.get_level_values(1)\n",
    "items = items.reindex(df_2017.index.get_level_values(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>2017-01-01 00:00:00</th>\n",
       "      <th>2017-01-02 00:00:00</th>\n",
       "      <th>2017-01-03 00:00:00</th>\n",
       "      <th>2017-01-04 00:00:00</th>\n",
       "      <th>2017-01-05 00:00:00</th>\n",
       "      <th>2017-01-06 00:00:00</th>\n",
       "      <th>2017-01-07 00:00:00</th>\n",
       "      <th>2017-01-08 00:00:00</th>\n",
       "      <th>2017-01-09 00:00:00</th>\n",
       "      <th>2017-01-10 00:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-08-06 00:00:00</th>\n",
       "      <th>2017-08-07 00:00:00</th>\n",
       "      <th>2017-08-08 00:00:00</th>\n",
       "      <th>2017-08-09 00:00:00</th>\n",
       "      <th>2017-08-10 00:00:00</th>\n",
       "      <th>2017-08-11 00:00:00</th>\n",
       "      <th>2017-08-12 00:00:00</th>\n",
       "      <th>2017-08-13 00:00:00</th>\n",
       "      <th>2017-08-14 00:00:00</th>\n",
       "      <th>2017-08-15 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>96995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99197</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103520</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103665</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105574</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "date                2017-01-01  2017-01-02  2017-01-03  2017-01-04  \\\n",
       "store_nbr item_nbr                                                   \n",
       "1         96995            0.0    0.000000    0.000000    0.000000   \n",
       "          99197            0.0    0.000000    1.386294    0.693147   \n",
       "          103520           0.0    0.693147    1.098612    0.000000   \n",
       "          103665           0.0    0.000000    0.000000    1.386294   \n",
       "          105574           0.0    0.000000    1.791759    2.564949   \n",
       "\n",
       "date                2017-01-05  2017-01-06  2017-01-07  2017-01-08  \\\n",
       "store_nbr item_nbr                                                   \n",
       "1         96995       0.000000    0.000000    0.000000    0.000000   \n",
       "          99197       0.693147    0.693147    1.098612    0.000000   \n",
       "          103520      1.098612    1.386294    0.693147    0.000000   \n",
       "          103665      1.098612    1.098612    0.693147    1.098612   \n",
       "          105574      2.302585    1.945910    1.609438    1.098612   \n",
       "\n",
       "date                2017-01-09  2017-01-10     ...      2017-08-06  \\\n",
       "store_nbr item_nbr                             ...                   \n",
       "1         96995       0.000000    0.000000     ...        1.098612   \n",
       "          99197       0.000000    0.693147     ...        0.000000   \n",
       "          103520      0.693147    0.693147     ...        0.000000   \n",
       "          103665      0.000000    2.079442     ...        0.693147   \n",
       "          105574      1.386294    2.302585     ...        0.000000   \n",
       "\n",
       "date                2017-08-07  2017-08-08  2017-08-09  2017-08-10  \\\n",
       "store_nbr item_nbr                                                   \n",
       "1         96995       1.098612    0.000000    0.000000    0.693147   \n",
       "          99197       1.098612    0.000000    1.098612    0.000000   \n",
       "          103520      0.000000    1.386294    0.000000    1.386294   \n",
       "          103665      1.098612    0.000000    2.079442    2.302585   \n",
       "          105574      1.791759    2.079442    1.945910    2.397895   \n",
       "\n",
       "date                2017-08-11  2017-08-12  2017-08-13  2017-08-14  2017-08-15  \n",
       "store_nbr item_nbr                                                              \n",
       "1         96995       0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "          99197       0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "          103520      0.693147    0.693147    0.693147    0.000000    0.000000  \n",
       "          103665      1.098612    0.000000    0.000000    0.693147    0.693147  \n",
       "          105574      1.791759    1.791759    0.000000    1.386294    1.609438  \n",
       "\n",
       "[5 rows x 227 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Retrieve the data over the specified time period with a certain frequency\n",
    "\"\"\"\n",
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "\n",
    "\"\"\"\n",
    "    Retrieve temporal features from preprocessed data ::\n",
    "    \n",
    "    Sales -      current value, mean values over n days, square mean values,\n",
    "                 cumulative sales over n days, standard deviation in sales,\n",
    "                 mean, std and squared mean for the last same n weekdays;\n",
    "                 \n",
    "    Promotions - same as for sales;\n",
    "    \n",
    "    Oil prices - mean, cumulative and deviation of oil prices over last n days;\n",
    "\"\"\"\n",
    "\n",
    "def prepare_dataset(t2017, is_train=True):\n",
    "    X = pd.DataFrame({\n",
    "        \"day_1_2017\": get_timespan(df_2017, t2017, 1, 1).values.ravel(),\n",
    "        \"mean_3_2017\": get_timespan(df_2017, t2017, 3, 3).mean(axis=1).values,\n",
    "        \"SQUARE_mean_3_2017\": get_timespan(df_2017, t2017, 3, 3).mean(axis=1).values ** 2,\n",
    "        #\"CUMULATIVE_3\": get_timespan(df_2017, t2017, 3, 3).sum(axis=1).values,\n",
    "        \"std_3_2017\": get_timespan(df_2017, t2017, 3, 3).std(axis=1).values,\n",
    "        \"mean_7_2017\": get_timespan(df_2017, t2017, 7, 7).mean(axis=1).values,\n",
    "        \"CUMULATIVE_7\": get_timespan(df_2017, t2017, 7, 7).sum(axis=1).values,\n",
    "        \"SQUARE_mean_7_2017\": get_timespan(df_2017, t2017, 7, 7).mean(axis=1).values ** 2,\n",
    "        \"std_7_2017\": get_timespan(df_2017, t2017, 7, 7).std(axis=1).values,\n",
    "        \"SQUARE_std_7_2017\": get_timespan(df_2017, t2017, 7, 7).std(axis=1).values ** 2,\n",
    "        \"mean_14_2017\": get_timespan(df_2017, t2017, 14, 14).mean(axis=1).values,\n",
    "        \"SQUARE_mean_14_2017\": get_timespan(df_2017, t2017, 14, 14).mean(axis=1).values ** 2,\n",
    "        \"std_14_2017\": get_timespan(df_2017, t2017, 14, 14).std(axis=1).values,\n",
    "        \"CUMULATIVE_14\": get_timespan(df_2017, t2017, 14, 14).sum(axis=1).values,\n",
    "        \"SQUARE_std_14_2017\": get_timespan(df_2017, t2017, 14, 14).std(axis=1).values ** 2,\n",
    "        \n",
    "        \"mean_21_2017\": get_timespan(df_2017, t2017, 21, 21).mean(axis=1).values,\n",
    "        \"SQUARE_mean_21_2017\": get_timespan(df_2017, t2017, 21, 21).mean(axis=1).values ** 2,\n",
    "        \"std_21_2017\": get_timespan(df_2017, t2017, 21, 21).std(axis=1).values,\n",
    "        #\"CUMULATIVE_21\": get_timespan(df_2017, t2017, 21, 21).sum(axis=1).values,\n",
    "        #\"SQUARE_std_21_2017\": get_timespan(df_2017, t2017, 21, 21).std(axis=1).values ** 2,\n",
    "        \n",
    "        \"mean_30_2017\": get_timespan(df_2017, t2017, 30, 30).mean(axis=1).values,\n",
    "        \"SQUARE_mean_30_2017\": get_timespan(df_2017, t2017, 30, 30).mean(axis=1).values ** 2,\n",
    "        \"std_30_2017\": get_timespan(df_2017, t2017, 30, 30).std(axis=1).values,\n",
    "        \"CUMULATIVE_30\": get_timespan(df_2017, t2017, 30, 30).sum(axis=1).values,\n",
    "        \"mean_60_2017\": get_timespan(df_2017, t2017, 60, 60).mean(axis=1).values,\n",
    "        \"CUMULATIVE_60\": get_timespan(df_2017, t2017, 60, 60).sum(axis=1).values,\n",
    "        \"std_60_2017\": get_timespan(df_2017, t2017, 60, 60).std(axis=1).values,\n",
    "        \"mean_90_2017\": get_timespan(df_2017, t2017, 90, 90).mean(axis=1).values,\n",
    "        \"std_90_2017\": get_timespan(df_2017, t2017, 90, 90).std(axis=1).values,\n",
    "        #\"mean_140_2017\": get_timespan(df_2017, t2017, 140, 140).mean(axis=1).values,\n",
    "        #\"std_140_2017\": get_timespan(df_2017, t2017, 140, 140).std(axis=1).values,\n",
    "        \n",
    "        ## PROMO _____________________________________________________________\n",
    "        \n",
    "        \"promo_1_2017\": get_timespan(promo_2017, t2017, 1, 1).values.ravel(),\n",
    "        #\"promo_3_2017\": get_timespan(promo_2017, t2017, 3, 3).sum(axis=1).values,\n",
    "        #\"AVG_PROM_3\": get_timespan(promo_2017, t2017, 3, 3).mean(axis=1).values,\n",
    "        \"proSTD_3_2017\": get_timespan(promo_2017, t2017, 3, 3).std(axis=1).values,\n",
    "        \"promo_7_2017\": get_timespan(promo_2017, t2017, 7, 7).sum(axis=1).values,\n",
    "        \"proSTD_7_2017\": get_timespan(promo_2017, t2017, 7, 7).std(axis=1).values,\n",
    "        \"AVG_PROM_7\": get_timespan(promo_2017, t2017, 7, 7).mean(axis=1).values,\n",
    "        \"promo_14_2017\": get_timespan(promo_2017, t2017, 14, 14).sum(axis=1).values,\n",
    "        #\"AVG_PROM_14\": get_timespan(promo_2017, t2017, 14, 14).mean(axis=1).values,\n",
    "        \n",
    "        \"proSTD_14_2017\": get_timespan(promo_2017, t2017, 14, 14).std(axis=1).values,\n",
    "        #\"promo_21_2017\": get_timespan(promo_2017, t2017, 21, 21).sum(axis=1).values,\n",
    "        #\"AVG_PROM_21\": get_timespan(promo_2017, t2017, 21, 21).mean(axis=1).values,\n",
    "        \"proSTD_21_2017\": get_timespan(promo_2017, t2017, 21, 21).std(axis=1).values,\n",
    "        \"promo_60_2017\": get_timespan(promo_2017, t2017, 60, 60).sum(axis=1).values,\n",
    "        #\"AVG_PROM_60\": get_timespan(promo_2017, t2017, 60, 60).mean(axis=1).values,\n",
    "        #\"promo_90_2017\": get_timespan(promo_2017, t2017, 90, 90).sum(axis=1).values,\n",
    "        \"promo_140_2017\": get_timespan(promo_2017, t2017, 140, 140).sum(axis=1).values,\n",
    "        \n",
    "        ## OIL _____________________________________________________________\n",
    "        \n",
    "        \"NAFTA_30_2017\": get_timespan(oil_2017, t2017, 21, 21).mean(axis=1).values,\n",
    "        #\"NAFTA_60_2017\": get_timespan(oil_2017, t2017, 60, 60).mean(axis=1).values,\n",
    "        #\"NAFTA_90_2017\": get_timespan(oil_2017, t2017, 90, 90).mean(axis=1).values,\n",
    "        \n",
    "        \"CUM_NAFTA_30_2017\": get_timespan(oil_2017, t2017, 21, 21).sum(axis=1).values,\n",
    "        #\"CUM_NAFTA_60_2017\": get_timespan(oil_2017, t2017, 60, 60).sum(axis=1).values,\n",
    "        #\"CUM_NAFTA_90_2017\": get_timespan(oil_2017, t2017, 90, 90).sum(axis=1).values,\n",
    "        \n",
    "        \"STD_NAFTA_30_2017\": get_timespan(oil_2017, t2017, 30, 30).std(axis=1).values,\n",
    "        #\"STD_NAFTA_60_2017\": get_timespan(oil_2017, t2017, 60, 60).std(axis=1).values,\n",
    "    })\n",
    "    for i in range(7):\n",
    "        X['promo_4_dow{}_2017'.format(i)] = get_timespan(promo_2017, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        #X['SQUARE_prAWmo_4_dow{}_2017'.format(i)] = get_timespan(promo_2017, t2017, 28-i, 4, freq='7D').mean(axis=1).values ** 2\n",
    "        #X['SQUARE_prAWmo_12_dow{}_2017'.format(i)] = get_timespan(promo_2017, t2017, 84-i, 12, freq='7D').mean(axis=1).values ** 2\n",
    "        #X['promo_12_dow{}_2017'.format(i)] = get_timespan(promo_2017, t2017, 84-i, 12, freq='7D').mean(axis=1).values\n",
    "        #X['promo_20_dow{}_2017'.format(i)] = get_timespan(promo_2017, t2017, 140-i, 20, freq='7D').mean(axis=1).values\n",
    "        X['mean_2_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 14-i, 2, freq='7D').mean(axis=1).values\n",
    "        #X['SQUARE_mean_2_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 14-i, 2, freq='7D').mean(axis=1).values ** 2\n",
    "        X['STD_2_DOW{}_2017'.format(i)] = get_timespan(df_2017, t2017, 14-i, 2, freq='7D').std(axis=1).values\n",
    "        X['mean_4_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        #X['SQUARE_mean_4_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 28-i, 4, freq='7D').mean(axis=1).values ** 2\n",
    "        #X['STD_4_DOW{}_2017'.format(i)] = get_timespan(df_2017, t2017, 28-i, 4, freq='7D').std(axis=1).values\n",
    "        #X['mean_8_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 56-i, 8, freq='7D').mean(axis=1).values\n",
    "        #X['STD_8_DOW{}_2017'.format(i)] = get_timespan(df_2017, t2017, 56-i, 8, freq='7D').std(axis=1).values\n",
    "        X['SQUARE_mean_8_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 56-i, 8, freq='7D').mean(axis=1).values ** 2\n",
    "        X['mean_12_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 84-i, 12, freq='7D').mean(axis=1).values\n",
    "        #X['STD_12_DOW{}_2017'.format(i)] = get_timespan(df_2017, t2017, 84-i, 12, freq='7D').std(axis=1).values\n",
    "        #X['SQUARE_mean_12_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 84-i, 12, freq='7D').mean(axis=1).values ** 2\n",
    "        X['mean_16_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 112-i, 16, freq='7D').mean(axis=1).values\n",
    "        #X['STD_16_DOW{}_2017'.format(i)] = get_timespan(df_2017, t2017, 112-i, 16, freq='7D').std(axis=1).values\n",
    "        #X['mean_20_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 140-i, 20, freq='7D').mean(axis=1).values\n",
    "        #X['STD_20_DOW{}_2017'.format(i)] = get_timespan(df_2017, t2017, 140-i, 20, freq='7D').std(axis=1).values               \n",
    "    if is_train:\n",
    "        y = df_2017[\n",
    "            pd.date_range(t2017, periods=16)\n",
    "        ].values\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "t2017 = date(2017, 5, 31)\n",
    "X_l, y_l = [], []\n",
    "for i in range(6):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = prepare_dataset(\n",
    "        t2017 + delta\n",
    "    )\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l\n",
    "\n",
    "# Validation and test sets\n",
    "X_val, y_val = prepare_dataset(date(2017, 7, 26))\n",
    "X_test = prepare_dataset(date(2017, 8, 16), is_train=False)\n",
    "\n",
    "stores_items = pd.DataFrame(index=df_2017.index)\n",
    "test_ids = df_test[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM-specific preprocessing\n",
    "items = items.reindex( stores_items.index.get_level_values(1) )\n",
    "\n",
    "X_train = X_train.as_matrix()\n",
    "X_test = X_test.as_matrix()\n",
    "X_val = X_val.as_matrix()\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "### Tuned configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the number of features to determine the number of nodes\n",
    "nodes = X_train.shape[1]\n",
    "\n",
    "model.add(LSTM(nodes, input_shape=(X_train.shape[1],X_train.shape[2]), \n",
    "               activation = 'tanh'))\n",
    "model.add(Dropout(.12))\n",
    "model.add(Dense(nodes, activation = 'relu'))\n",
    "model.add(Dropout(.12))\n",
    "model.add(Dense(1, activation = 'relu'))\n",
    "model.compile(loss = 'msle', optimizer='adamax', metrics=['msle'])\n",
    "\n",
    "N_EPOCHS = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 1\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/25\n",
      " - 85s - loss: 0.1183 - mean_squared_logarithmic_error: 0.1122 - val_loss: 0.0951 - val_mean_squared_logarithmic_error: 0.0951\n",
      "Epoch 2/25\n",
      " - 84s - loss: 0.1033 - mean_squared_logarithmic_error: 0.0980 - val_loss: 0.0962 - val_mean_squared_logarithmic_error: 0.0962\n",
      "Epoch 3/25\n",
      " - 83s - loss: 0.1025 - mean_squared_logarithmic_error: 0.0972 - val_loss: 0.0972 - val_mean_squared_logarithmic_error: 0.0972\n",
      "Epoch 4/25\n",
      " - 83s - loss: 0.1021 - mean_squared_logarithmic_error: 0.0969 - val_loss: 0.0943 - val_mean_squared_logarithmic_error: 0.0943\n",
      "Epoch 5/25\n",
      " - 73s - loss: 0.1017 - mean_squared_logarithmic_error: 0.0966 - val_loss: 0.0936 - val_mean_squared_logarithmic_error: 0.0936\n",
      "Epoch 6/25\n",
      " - 74s - loss: 0.1015 - mean_squared_logarithmic_error: 0.0964 - val_loss: 0.0937 - val_mean_squared_logarithmic_error: 0.0937\n",
      "Epoch 7/25\n",
      " - 81s - loss: 0.1015 - mean_squared_logarithmic_error: 0.0964 - val_loss: 0.0950 - val_mean_squared_logarithmic_error: 0.0950\n",
      "Epoch 8/25\n",
      " - 83s - loss: 0.1013 - mean_squared_logarithmic_error: 0.0961 - val_loss: 0.0935 - val_mean_squared_logarithmic_error: 0.0935\n",
      "Epoch 9/25\n",
      " - 83s - loss: 0.1011 - mean_squared_logarithmic_error: 0.0960 - val_loss: 0.0939 - val_mean_squared_logarithmic_error: 0.0939\n",
      "Epoch 10/25\n",
      " - 85s - loss: 0.1010 - mean_squared_logarithmic_error: 0.0959 - val_loss: 0.0939 - val_mean_squared_logarithmic_error: 0.0939\n",
      "Epoch 11/25\n",
      " - 83s - loss: 0.1009 - mean_squared_logarithmic_error: 0.0958 - val_loss: 0.0937 - val_mean_squared_logarithmic_error: 0.0937\n",
      "Epoch 12/25\n",
      " - 84s - loss: 0.1008 - mean_squared_logarithmic_error: 0.0957 - val_loss: 0.0942 - val_mean_squared_logarithmic_error: 0.0942\n",
      "Epoch 13/25\n",
      " - 84s - loss: 0.1009 - mean_squared_logarithmic_error: 0.0957 - val_loss: 0.0940 - val_mean_squared_logarithmic_error: 0.0940\n",
      "Epoch 14/25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "\n",
    "# Misprediction of perishable products induces more losses\n",
    "sample_weights=np.array( pd.concat([items[\"perishable\"]] * 6) * 0.25 + 1 )\n",
    "\n",
    "for i in range(16):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Step %d\" % (i+1))\n",
    "    print(\"=\" * 50)\n",
    "    y = y_train[:, i]\n",
    "    xv = X_val\n",
    "    yv = y_val[:, i]\n",
    "    model.fit(X_train, y, batch_size = 512, epochs = N_EPOCHS, verbose=2,\n",
    "               sample_weight=sample_weights, validation_data=(xv,yv)) \n",
    "    val_pred.append(model.predict(X_val))\n",
    "    test_pred.append(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted validation mse:  0.392228189502\n",
      "Full validation mse:        0.391956139181\n",
      "'Public' validation mse:    0.360790351786\n",
      "'Private' validation mse:   0.406122406179\n"
     ]
    }
   ],
   "source": [
    "n_public = 5 # Number of days in public test set\n",
    "weights=pd.concat([items[\"perishable\"]]) * 0.25 + 1\n",
    "print(\"Unweighted validation mse: \", mean_squared_error(\n",
    "    y_val, np.array(val_pred).squeeze(axis=2).transpose()) )\n",
    "print(\"Full validation mse:       \", mean_squared_error(\n",
    "    y_val, np.array(val_pred).squeeze(axis=2).transpose(), sample_weight=weights) )\n",
    "print(\"'Public' validation mse:   \", mean_squared_error(\n",
    "    y_val[:,:n_public], np.array(val_pred).squeeze(axis=2).transpose()[:,:n_public], \n",
    "    sample_weight=weights) )\n",
    "print(\"'Private' validation mse:  \", mean_squared_error(\n",
    "    y_val[:,n_public:], np.array(val_pred).squeeze(axis=2).transpose()[:,n_public:], \n",
    "    sample_weight=weights) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the predictions submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = np.array(test_pred).squeeze(axis=2).transpose()\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=stores_items.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame(\"unit_sales\")\n",
    "df_preds.index.set_names([\"store_nbr\", \"item_nbr\", \"date\"], inplace=True)\n",
    "\n",
    "submission = test_ids.join(df_preds, how=\"left\").fillna(0)\n",
    "submission[\"unit_sales\"] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission.to_csv('LSTM.csv', float_format='%.5f', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
